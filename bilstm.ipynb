{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10256992,"sourceType":"datasetVersion","datasetId":6345042}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/cardiffnlp/tweeteval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:42.243666Z","iopub.execute_input":"2024-12-21T18:23:42.244082Z","iopub.status.idle":"2024-12-21T18:23:42.418035Z","shell.execute_reply.started":"2024-12-21T18:23:42.244055Z","shell.execute_reply":"2024-12-21T18:23:42.416899Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'tweeteval' already exists and is not an empty directory.\n","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"!pip install keras tensorflow\n\n!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:42.419932Z","iopub.execute_input":"2024-12-21T18:23:42.420251Z","iopub.status.idle":"2024-12-21T18:23:49.279339Z","shell.execute_reply.started":"2024-12-21T18:23:42.420214Z","shell.execute_reply":"2024-12-21T18:23:49.278158Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.8.1)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.12.1)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.16.0)\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Bidirectional\nfrom keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer # Changed import statement to use tensorflow.keras\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.281237Z","iopub.execute_input":"2024-12-21T18:23:49.281588Z","iopub.status.idle":"2024-12-21T18:23:49.286331Z","shell.execute_reply.started":"2024-12-21T18:23:49.281562Z","shell.execute_reply":"2024-12-21T18:23:49.285648Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# Define the dataset folder path\ndataset_path = \"//kaggle/working/tweeteval/datasets/emoji\"  # Replace this with the actual path if different\n\n# Function to load texts and labels from a file\ndef load_texts_and_labels(text_file, label_file):\n    # Read texts\n    with open(text_file, \"r\", encoding=\"utf-8\") as f:\n        texts = f.read().splitlines()\n\n    # Read labels\n    with open(label_file, \"r\", encoding=\"utf-8\") as f:\n        labels = f.read().splitlines()\n\n    # Return a list of tuples (text, label)\n    return list(zip(texts, labels))\n\n# Dictionary to hold the dataset\ndata = {}\n\n# Load train, validation, and test sets\ndata[\"train\"] = load_texts_and_labels(\n    os.path.join(dataset_path, \"train_text.txt\"),\n    os.path.join(dataset_path, \"train_labels.txt\")\n)\ndata[\"val\"] = load_texts_and_labels(\n    os.path.join(dataset_path, \"val_text.txt\"),\n    os.path.join(dataset_path, \"val_labels.txt\")\n)\ndata[\"test\"] = load_texts_and_labels(\n    os.path.join(dataset_path, \"test_text.txt\"),\n    os.path.join(dataset_path, \"val_labels.txt\")  # Assuming test labels are in `val_labels.txt`\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.287602Z","iopub.execute_input":"2024-12-21T18:23:49.287804Z","iopub.status.idle":"2024-12-21T18:23:49.411151Z","shell.execute_reply.started":"2024-12-21T18:23:49.287786Z","shell.execute_reply":"2024-12-21T18:23:49.410381Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# Convert data into DataFrames for train, validation, and test sets\ndef convert_to_dataframe(data_split):\n    return pd.DataFrame(data_split, columns=[\"TEXT\", \"Label\"])\n\n# Create DataFrames for train, validation, and test sets\ntrain_data = convert_to_dataframe(data[\"train\"])\nval_data = convert_to_dataframe(data[\"val\"])\ntest_data = convert_to_dataframe(data[\"test\"])\n\n# Display the first few rows of each DataFrame to confirm the format\nprint(\"Training Data:\")\nprint(train_data.head())\n\nprint(\"\\nValidation Data:\")\nprint(val_data.head())\n\nprint(\"\\nTest Data:\")\nprint(test_data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.412079Z","iopub.execute_input":"2024-12-21T18:23:49.412429Z","iopub.status.idle":"2024-12-21T18:23:49.437187Z","shell.execute_reply.started":"2024-12-21T18:23:49.412393Z","shell.execute_reply":"2024-12-21T18:23:49.436391Z"}},"outputs":[{"name":"stdout","text":"Training Data:\n                                                TEXT Label\n0  Sunday afternoon walking through Venice in the...    12\n1  Time for some BBQ and whiskey libations. Chomp...    19\n2  Love love love all these people ️ ️ ️ #friends...     0\n3                               ️ ️ ️ ️ @ Toys\"R\"Us      0\n4  Man these are the funniest kids ever!! That fa...     2\n\nValidation Data:\n                                                TEXT Label\n0  A little throwback with my favourite person @ ...     0\n1  glam on @user yesterday for #kcon makeup using...     7\n2  Democracy Plaza in the wake of a stunning outc...    11\n3   Then &amp; Now. VILO @ Walt Disney Magic Kingdom     0\n4               Who never... @ A Galaxy Far Far Away     2\n\nTest Data:\n                                                TEXT Label\n0                                  en Pelham Parkway     0\n1  The calm before...... | w/ sofarsounds @user |...     7\n2  Just witnessed the great solar eclipse @ Tampa...    11\n3  This little lady is 26 weeks pregnant today! E...     0\n4  Great road trip views! @ Shartlesville, Pennsy...     2\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"mapping_path = \"/kaggle/working/tweeteval/datasets/emoji/mapping.txt\"\n\n# Load the mapping file into a DataFrame\n# Assuming mapping.txt is formatted as tab-separated, with each line as: Index Emoji Description\nmapping_df = pd.read_csv(mapping_path, sep=\"\\t\", header=None, names=[\"index\", \"emoticons\", \"description\"])\n\n# Add a numeric \"number\" column that matches the row index\nmapping_df[\"number\"] = mapping_df.index\n\n# Rename columns to match the required format\nmapping_df = mapping_df.rename(columns={\"index\": \"Unnamed: 0\"})\n\n# Drop the \"description\" column if it's not needed\nmappings= mapping_df[[\"Unnamed: 0\", \"emoticons\", \"number\"]]\n\n# Display the first few rows to verify\nprint(mappings.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.438200Z","iopub.execute_input":"2024-12-21T18:23:49.438638Z","iopub.status.idle":"2024-12-21T18:23:49.451410Z","shell.execute_reply.started":"2024-12-21T18:23:49.438612Z","shell.execute_reply":"2024-12-21T18:23:49.450530Z"}},"outputs":[{"name":"stdout","text":"  Unnamed: 0                      emoticons  number\n0          ❤                    _red_heart_       0\n1          😍  _smiling_face_with_hearteyes_       1\n2          😂       _face_with_tears_of_joy_       2\n3          💕                   _two_hearts_       3\n4          🔥                         _fire_       4\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"train_data.shape, test_data.shape, mappings.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.452207Z","iopub.execute_input":"2024-12-21T18:23:49.452471Z","iopub.status.idle":"2024-12-21T18:23:49.474940Z","shell.execute_reply.started":"2024-12-21T18:23:49.452440Z","shell.execute_reply":"2024-12-21T18:23:49.474316Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"((45000, 2), (5000, 2), (20, 3))"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"train_length = train_data.shape[0]\ntest_length = test_data.shape[0]\ntrain_length, test_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.477608Z","iopub.execute_input":"2024-12-21T18:23:49.477828Z","iopub.status.idle":"2024-12-21T18:23:49.489956Z","shell.execute_reply.started":"2024-12-21T18:23:49.477809Z","shell.execute_reply":"2024-12-21T18:23:49.489127Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"(45000, 5000)"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"# import nltk\n\n# # Download the 'stopwords' dataset\n# nltk.download('stopwords')\n\n# from nltk.corpus import stopwords\n\n# stop_words = stopwords.words(\"english\")\n# stop_words[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.491805Z","iopub.execute_input":"2024-12-21T18:23:49.491990Z","iopub.status.idle":"2024-12-21T18:23:49.503754Z","shell.execute_reply.started":"2024-12-21T18:23:49.491974Z","shell.execute_reply":"2024-12-21T18:23:49.503059Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# tokenize the sentences\n# def tokenize(tweets):\n#     stop_words = stopwords.words(\"english\")\n#     tokenized_tweets = []\n#     for tweet in tweets:\n#         # split all words in the tweet\n#         words = tweet.split(\" \")\n#         tokenized_string = \"\"\n#         for word in words:\n#             # remove @handles -> useless -> no information\n#             if word and word[0] != '@' and word not in stop_words:\n#                 # if a hashtag, remove # -> adds no new information\n#                 if word[0] == \"#\":\n#                     word = word[1:]\n#                 tokenized_string += word + \" \"\n#         tokenized_tweets.append(tokenized_string)\n#     return tokenized_tweets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.504715Z","iopub.execute_input":"2024-12-21T18:23:49.504999Z","iopub.status.idle":"2024-12-21T18:23:49.518598Z","shell.execute_reply.started":"2024-12-21T18:23:49.504970Z","shell.execute_reply":"2024-12-21T18:23:49.517881Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"import requests\nimport re\n\ndef load_slang_dict(url):\n    \"\"\"\n    Load the slang dictionary from the provided URL.\n    Assumes the file contains each slang and its meaning separated by a hyphen (-).\n    \"\"\"\n    slang_dict = {}\n    response = requests.get(url)\n    if response.status_code == 200:\n        lines = response.text.splitlines()\n        for line in lines:\n            if '-' in line:  # Assumes the format is \"slang - meaning\"\n                parts = line.split('-', 1)  # Split on the first hyphen\n                if len(parts) == 2:\n                    slang, meaning = parts\n                    slang_dict[slang.strip().lower()] = meaning.strip()\n    else:\n        raise Exception(f\"Failed to fetch slang dictionary from {url}. HTTP {response.status_code}\")\n    return slang_dict\n\ndef expand_slangs(word, slang_dict):\n    \"\"\"\n    Replace slangs with their expanded forms based on a given dictionary.\n    \"\"\"\n    return slang_dict.get(word.lower(), word)\n\ndef reduce_elongated_words(word):\n    \"\"\"\n    Reduce elongated words (e.g., 'soooo' -> 'so').\n    \"\"\"\n    return re.sub(r\"(.)\\1{2,}\", r\"\\1\", word)\n\ndef tokenize(tweets, slang_dict):\n    \"\"\"\n    Tokenizes tweets, handles slangs, reduces elongated words,\n    and retains @ mentions and stopwords.\n    \"\"\"\n    tokenized_tweets = []\n    \n    for tweet in tweets:\n        # Split words in the tweet\n        words = tweet.split()\n        processed_words = []\n        \n        for word in words:\n            # Remove URLs\n            word = re.sub(r\"http\\S+|www\\S+\", \"\", word)\n            \n            # Remove hashtags (#), but keep the word\n            if word.startswith(\"#\"):\n                word = word[1:]\n            \n            # Expand slangs\n            word = expand_slangs(word, slang_dict)\n            \n            # Reduce elongated words\n            word = reduce_elongated_words(word)\n            \n            # Keep @ mentions and punctuation intact\n            processed_words.append(word)\n        \n        tokenized_tweets.append(processed_words)\n    \n    return tokenized_tweets\n\n# Load the slang dictionary from the GitHub URL\nslang_dict_url = \"https://raw.githubusercontent.com/haierlord/resource/master/slangs\"\nslang_dict = load_slang_dict(slang_dict_url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.519439Z","iopub.execute_input":"2024-12-21T18:23:49.519633Z","iopub.status.idle":"2024-12-21T18:23:49.591136Z","shell.execute_reply.started":"2024-12-21T18:23:49.519616Z","shell.execute_reply":"2024-12-21T18:23:49.590322Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# translate tweets to a sequence of numbers\ndef encod_tweets(tweets):\n    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', split=\" \", lower=True)\n    tokenizer.fit_on_texts(tweets)\n    return tokenizer, tokenizer.texts_to_sequences(tweets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.592012Z","iopub.execute_input":"2024-12-21T18:23:49.592209Z","iopub.status.idle":"2024-12-21T18:23:49.596091Z","shell.execute_reply.started":"2024-12-21T18:23:49.592192Z","shell.execute_reply":"2024-12-21T18:23:49.595144Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# apply padding to dataset and convert labels to bitmaps\ndef format_data(encoded_tweets, max_length, labels):\n    x = pad_sequences(encoded_tweets, maxlen= max_length, padding='post', truncating='post')\n    y = []\n    for emoji in labels:\n        emoji_index = int(emoji)\n        bit_vec = np.zeros(20)\n        bit_vec[emoji_index] = 1\n        y.append(bit_vec)\n    y = np.asarray(y)\n    return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.596894Z","iopub.execute_input":"2024-12-21T18:23:49.597153Z","iopub.status.idle":"2024-12-21T18:23:49.613573Z","shell.execute_reply.started":"2024-12-21T18:23:49.597134Z","shell.execute_reply":"2024-12-21T18:23:49.612693Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# create weight matrix from pre trained embeddings\ndef create_weight_matrix(vocab, raw_embeddings):\n    vocab_size = len(vocab) + 1\n    weight_matrix = np.zeros((vocab_size, 300))\n    for word, idx in vocab.items():\n        if word in raw_embeddings:\n            weight_matrix[idx] = raw_embeddings[word]\n    return weight_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.614595Z","iopub.execute_input":"2024-12-21T18:23:49.614872Z","iopub.status.idle":"2024-12-21T18:23:49.627545Z","shell.execute_reply.started":"2024-12-21T18:23:49.614846Z","shell.execute_reply":"2024-12-21T18:23:49.626654Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, Attention, Layer, Concatenate\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau\nfrom sklearn.metrics import f1_score\nimport numpy as np\nimport tensorflow.keras.backend as K \n\nclass F1ScoreCallback(Callback):\n    def __init__(self, x_val, y_val):\n        super(F1ScoreCallback, self).__init__()\n        self.x_val = x_val\n        self.y_val = y_val\n\n    def on_epoch_end(self, epoch, logs=None):\n        y_pred = np.argmax(self.model.predict(self.x_val), axis=-1)\n        y_true = np.argmax(self.y_val, axis=-1)\n        f1 = f1_score(y_true, y_pred, average='macro')\n        print(f' - val_f1: {f1}')\n        logs['val_f1'] = f1\n\n# Attention Layer\nclass AttentionLayer(Layer):\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]), initializer=\"random_normal\", trainable=True)\n        self.b = self.add_weight(shape=(input_shape[-1],), initializer=\"zeros\", trainable=True)\n        self.u = self.add_weight(shape=(input_shape[-1], 1), initializer=\"random_normal\", trainable=True)\n        super(AttentionLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        q = K.tanh(K.dot(inputs, self.W) + self.b) \n        a = K.dot(q, self.u) \n        a = K.squeeze(a, -1)\n        a = K.softmax(a)\n        a = K.expand_dims(a, -1)\n        output = inputs * a\n        return K.sum(output, axis=1)\n\n# Modified model\ndef final_model(weight_matrix, vocab_size, max_length, x_train, y_train, x_val, y_val, embedding_dim=300, lstm_units=512, epochs=5, learning_rate=0.01):\n    # Embedding layer\n    embedding_layer = Embedding(\n        input_dim=vocab_size,\n        output_dim=embedding_dim,\n        weights=[weight_matrix],\n        input_length=max_length,\n        trainable=True,\n        mask_zero=False\n    )\n\n    # Model architecture with BiLSTM and Attention\n    model = Sequential()\n    model.add(embedding_layer)\n    \n    model.add(Dropout(0.3))\n    \n    # Bidirectional LSTM layer\n    model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n    \n    # Attention mechanism\n    model.add(AttentionLayer())\n    \n    # MLP layer for final classification\n    model.add(Dense(200, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(20, activation='softmax'))  # Assuming 20 classes for emoji classification\n    \n    # Compile model with Adam optimizer and learning rate\n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    # Callbacks\n    lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n    f1_callback = F1ScoreCallback(x_val, y_val)\n\n    # Train the model\n    model.fit(\n        x_train, y_train,\n        epochs=epochs,\n        validation_data=(x_val, y_val),\n        callbacks=[lr_reduction, f1_callback]\n    )\n\n    # Evaluate model on validation set\n    score = model.evaluate(x_val, y_val)\n    \n    return model, score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.659582Z","iopub.execute_input":"2024-12-21T18:23:49.659815Z","iopub.status.idle":"2024-12-21T18:23:49.680945Z","shell.execute_reply.started":"2024-12-21T18:23:49.659797Z","shell.execute_reply":"2024-12-21T18:23:49.680065Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"import math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.681858Z","iopub.execute_input":"2024-12-21T18:23:49.682132Z","iopub.status.idle":"2024-12-21T18:23:49.699183Z","shell.execute_reply.started":"2024-12-21T18:23:49.682112Z","shell.execute_reply":"2024-12-21T18:23:49.698553Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"tokenized_tweets = tokenize(train_data['TEXT'], slang_dict)\ntokenized_tweets += tokenize(test_data['TEXT'], slang_dict)\n# max_length = math.ceil(sum([len(s.split(\" \")) for s in tokenized_tweets])/len(tokenized_tweets))\n# tokenizer, encoded_tweets = encod_tweets(tokenized_tweets)\n# max_length, len(tokenized_tweets)\n\n# Calculate the average token length per tweet\naverage_length = sum([len(tokens) for tokens in tokenized_tweets]) / len(tokenized_tweets)\nmax_length = math.ceil(average_length)\n\n# Encode tweets\ntokenizer, encoded_tweets = encod_tweets(tokenized_tweets)\n\n# Display results\nmax_length, len(tokenized_tweets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:25:36.940171Z","iopub.execute_input":"2024-12-21T18:25:36.940517Z","iopub.status.idle":"2024-12-21T18:25:39.861956Z","shell.execute_reply.started":"2024-12-21T18:25:36.940490Z","shell.execute_reply":"2024-12-21T18:25:39.861014Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"(12, 50000)"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"x, y = format_data(encoded_tweets[:train_length], max_length, train_data['Label'])\nlen(x), len(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:25:47.797490Z","iopub.execute_input":"2024-12-21T18:25:47.797807Z","iopub.status.idle":"2024-12-21T18:25:47.980830Z","shell.execute_reply.started":"2024-12-21T18:25:47.797784Z","shell.execute_reply":"2024-12-21T18:25:47.979809Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"(45000, 45000)"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"x_test, y_test = format_data(encoded_tweets[train_length:], max_length, test_data['Label'])\nlen(x_test), len(y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:25:53.316671Z","iopub.execute_input":"2024-12-21T18:25:53.316966Z","iopub.status.idle":"2024-12-21T18:25:53.351402Z","shell.execute_reply.started":"2024-12-21T18:25:53.316946Z","shell.execute_reply":"2024-12-21T18:25:53.350695Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"(5000, 5000)"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"vocab = tokenizer.word_index\n# vocab, len(vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:26:05.003418Z","iopub.execute_input":"2024-12-21T18:26:05.003783Z","iopub.status.idle":"2024-12-21T18:26:05.009955Z","shell.execute_reply.started":"2024-12-21T18:26:05.003758Z","shell.execute_reply":"2024-12-21T18:26:05.008988Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"from gensim.models.keyedvectors import KeyedVectors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:26:07.855865Z","iopub.execute_input":"2024-12-21T18:26:07.856168Z","iopub.status.idle":"2024-12-21T18:26:07.860099Z","shell.execute_reply.started":"2024-12-21T18:26:07.856145Z","shell.execute_reply":"2024-12-21T18:26:07.859199Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"word2vec_model = KeyedVectors.load_word2vec_format('/kaggle/input/swm-wordembed/model_swm_300-6-10-low.w2v', binary=False)\n\n# Create the weight matrix\nweight_matrix = create_weight_matrix(vocab, word2vec_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:26:09.506374Z","iopub.execute_input":"2024-12-21T18:26:09.506660Z","iopub.status.idle":"2024-12-21T18:26:48.524179Z","shell.execute_reply.started":"2024-12-21T18:26:09.506641Z","shell.execute_reply":"2024-12-21T18:26:48.523208Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"model, score= final_model(weight_matrix, len(vocab)+1, max_length, x, y, x_test, y_test, epochs = 50)\nmodel, score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model in HDF5 format\nmodel.save(\"final_model.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.760031Z","iopub.status.idle":"2024-12-21T18:23:49.760378Z","shell.execute_reply":"2024-12-21T18:23:49.760228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout, Concatenate\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.preprocessing.sequence import pad_sequences\n# from keras.layers import Layer\n# import tensorflow.keras.backend as K\n# import numpy as np\n\n# def create_final_model(\n#     word_embedding_matrix, vocab_size, max_length,\n#     char_vocab_size, pos_vocab_size, ner_vocab_size,\n#     word_embedding_dim=300, char_embedding_dim=50, pos_embedding_dim=50, ner_embedding_dim=50,\n#     lstm_hidden_size=512, mlp_hidden_size=200, learning_rate=0.01\n# ):\n#     # Custom Attention Layer\n#     class CustomAttention(Layer):\n#         def __init__(self, **kwargs):\n#             super(CustomAttention, self).__init__(**kwargs)\n        \n#         def build(self, input_shape):\n#             self.attention_weights = self.add_weight(\n#                 name=\"attention_weights\",\n#                 shape=(input_shape[-1], 1),\n#                 initializer=\"random_normal\",\n#                 trainable=True,\n#             )\n#             super(CustomAttention, self).build(input_shape)\n        \n#         def call(self, inputs):\n#             attention_logits = K.dot(inputs, self.attention_weights)  # Compute attention logits\n#             attention_logits = K.squeeze(attention_logits, axis=-1)  # Remove the extra dimension\n#             attention_scores = K.softmax(attention_logits)  # Compute attention scores\n#             weighted_input = inputs * K.expand_dims(attention_scores, axis=-1)  # Apply attention scores\n#             output = K.sum(weighted_input, axis=1)  # Aggregate\n#             return output\n\n#         def compute_output_shape(self, input_shape):\n#             return (input_shape[0], input_shape[-1])\n\n#     # Input layers for word, char, POS, and NER embeddings\n#     word_input = Input(shape=(max_length,), name=\"word_input\")\n#     char_input = Input(shape=(max_length,), name=\"char_input\")\n#     pos_input = Input(shape=(max_length,), name=\"pos_input\")\n#     ner_input = Input(shape=(max_length,), name=\"ner_input\")\n\n#     # Word embedding layer using pre-trained word embeddings\n#     word_embedding_layer = Embedding(\n#         input_dim=vocab_size,\n#         output_dim=word_embedding_dim,\n#         weights=[word_embedding_matrix],\n#         input_length=max_length,\n#         trainable=False,\n#         mask_zero=True\n#     )(word_input)\n\n#     # Char, POS, and NER embedding layers\n#     char_embedding_layer = Embedding(\n#         input_dim=char_vocab_size,\n#         output_dim=char_embedding_dim,\n#         input_length=max_length,\n#         trainable=True\n#     )(char_input)\n\n#     pos_embedding_layer = Embedding(\n#         input_dim=pos_vocab_size,\n#         output_dim=pos_embedding_dim,\n#         input_length=max_length,\n#         trainable=True\n#     )(pos_input)\n\n#     ner_embedding_layer = Embedding(\n#         input_dim=ner_vocab_size,\n#         output_dim=ner_embedding_dim,\n#         input_length=max_length,\n#         trainable=True\n#     )(ner_input)\n\n#     # Concatenate all embeddings\n#     concatenated_embeddings = Concatenate(axis=-1)([\n#         word_embedding_layer, char_embedding_layer, pos_embedding_layer, ner_embedding_layer\n#     ])\n\n#     # Bi-Directional LSTM to process concatenated embeddings\n#     bi_lstm_output = Bidirectional(LSTM(\n#         lstm_hidden_size,\n#         return_sequences=True\n#     ))(concatenated_embeddings)\n\n#     # Custom Attention Mechanism\n#     sentence_representation = CustomAttention()(bi_lstm_output)\n\n#     # Multi-layer Perceptron (MLP)\n#     dense_layer = Dense(mlp_hidden_size, activation=\"relu\")(sentence_representation)\n#     output_layer = Dense(20, activation=\"softmax\", name=\"output_layer\")(dense_layer)\n\n#     # Compile the model\n#     model = Model(inputs=[word_input, char_input, pos_input, ner_input], outputs=output_layer)\n#     optimizer = Adam(learning_rate=learning_rate)\n#     model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n#     return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.761336Z","iopub.status.idle":"2024-12-21T18:23:49.761690Z","shell.execute_reply":"2024-12-21T18:23:49.761532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# # Helper function to preprocess sequences\n# def encode_and_pad(sequences, tokenizer, max_length):\n#     encoded_sequences = tokenizer.texts_to_sequences(sequences)\n#     padded_sequences = pad_sequences(encoded_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n#     return padded_sequences\n\n# # Tokenizers for each type\n# word_tokenizer = Tokenizer()\n# char_tokenizer = Tokenizer()\n# pos_tokenizer = Tokenizer()\n# ner_tokenizer = Tokenizer()\n\n# # Fit tokenizers\n# word_tokenizer.fit_on_texts(data[\"train\"][\"words\"])  # Tokenize training words\n# char_tokenizer.fit_on_texts(data[\"train\"][\"chars\"])  # Tokenize training characters\n# pos_tokenizer.fit_on_texts(data[\"train\"][\"pos\"])    # Tokenize POS tags\n# ner_tokenizer.fit_on_texts(data[\"train\"][\"ner\"])    # Tokenize NER tags\n\n# # Create training inputs\n# x_train_words = encode_and_pad(data[\"train\"][\"words\"], word_tokenizer, max_length)\n# x_train_chars = encode_and_pad(data[\"train\"][\"chars\"], char_tokenizer, max_length)\n# x_train_pos = encode_and_pad(data[\"train\"][\"pos\"], pos_tokenizer, max_length)\n# x_train_ner = encode_and_pad(data[\"train\"][\"ner\"], ner_tokenizer, max_length)\n\n# # Create validation inputs\n# x_val_words = encode_and_pad(data[\"val\"][\"words\"], word_tokenizer, max_length)\n# x_val_chars = encode_and_pad(data[\"val\"][\"chars\"], char_tokenizer, max_length)\n# x_val_pos = encode_and_pad(data[\"val\"][\"pos\"], pos_tokenizer, max_length)\n# x_val_ner = encode_and_pad(data[\"val\"][\"ner\"], ner_tokenizer, max_length)\n\n# # Create test inputs\n# x_test_words = encode_and_pad(data[\"test\"][\"words\"], word_tokenizer, max_length)\n# x_test_chars = encode_and_pad(data[\"test\"][\"chars\"], char_tokenizer, max_length)\n# x_test_pos = encode_and_pad(data[\"test\"][\"pos\"], pos_tokenizer, max_length)\n# x_test_ner = encode_and_pad(data[\"test\"][\"ner\"], ner_tokenizer, max_length)\n\n# # Labels\n# y_train = data[\"train\"][\"labels\"]\n# y_val = data[\"val\"][\"labels\"]\n# y_test = data[\"test\"][\"labels\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.762523Z","iopub.status.idle":"2024-12-21T18:23:49.762830Z","shell.execute_reply":"2024-12-21T18:23:49.762685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# # Summarize the model\n# model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T18:23:49.765823Z","iopub.status.idle":"2024-12-21T18:23:49.766123Z","shell.execute_reply":"2024-12-21T18:23:49.766014Z"}},"outputs":[],"execution_count":null}]}