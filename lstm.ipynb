{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10050915,"sourceType":"datasetVersion","datasetId":6032820},{"sourceId":10256992,"sourceType":"datasetVersion","datasetId":6345042}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/cardiffnlp/tweeteval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:48:56.672575Z","iopub.execute_input":"2024-12-22T13:48:56.672885Z","iopub.status.idle":"2024-12-22T13:48:58.084573Z","shell.execute_reply.started":"2024-12-22T13:48:56.672859Z","shell.execute_reply":"2024-12-22T13:48:58.083493Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'tweeteval'...\nremote: Enumerating objects: 370, done.\u001b[K\nremote: Counting objects: 100% (16/16), done.\u001b[K\nremote: Compressing objects: 100% (15/15), done.\u001b[K\nremote: Total 370 (delta 13), reused 3 (delta 1), pack-reused 354 (from 1)\u001b[K\nReceiving objects: 100% (370/370), 8.49 MiB | 27.43 MiB/s, done.\nResolving deltas: 100% (122/122), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install keras tensorflow\n\n!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:48:59.488828Z","iopub.execute_input":"2024-12-22T13:48:59.489115Z","iopub.status.idle":"2024-12-22T13:49:06.974298Z","shell.execute_reply.started":"2024-12-22T13:48:59.489095Z","shell.execute_reply":"2024-12-22T13:49:06.973418Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.8.1)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.12.1)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.16.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer # Changed import statement to use tensorflow.keras\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:12.896716Z","iopub.execute_input":"2024-12-22T13:49:12.897032Z","iopub.status.idle":"2024-12-22T13:49:20.462282Z","shell.execute_reply.started":"2024-12-22T13:49:12.897007Z","shell.execute_reply":"2024-12-22T13:49:20.461627Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Define the dataset folder path\ndataset_path = \"//kaggle/working/tweeteval/datasets/emoji\"  # Replace this with the actual path if different\n\n# Function to load texts and labels from a file\ndef load_texts_and_labels(text_file, label_file):\n    # Read texts\n    with open(text_file, \"r\", encoding=\"utf-8\") as f:\n        texts = f.read().splitlines()\n\n    # Read labels\n    with open(label_file, \"r\", encoding=\"utf-8\") as f:\n        labels = f.read().splitlines()\n\n    # Return a list of tuples (text, label)\n    return list(zip(texts, labels))\n\n# Dictionary to hold the dataset\ndata = {}\n\n# Load train, validation, and test sets\ndata[\"train\"] = load_texts_and_labels(\n    os.path.join(dataset_path, \"train_text.txt\"),\n    os.path.join(dataset_path, \"train_labels.txt\")\n)\ndata[\"val\"] = load_texts_and_labels(\n    os.path.join(dataset_path, \"val_text.txt\"),\n    os.path.join(dataset_path, \"val_labels.txt\")\n)\ndata[\"test\"] = load_texts_and_labels(\n    os.path.join(dataset_path, \"test_text.txt\"),\n    os.path.join(dataset_path, \"val_labels.txt\")  # Assuming test labels are in `val_labels.txt`\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:22.566059Z","iopub.execute_input":"2024-12-22T13:49:22.566709Z","iopub.status.idle":"2024-12-22T13:49:22.674940Z","shell.execute_reply.started":"2024-12-22T13:49:22.566677Z","shell.execute_reply":"2024-12-22T13:49:22.674275Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Convert data into DataFrames for train, validation, and test sets\ndef convert_to_dataframe(data_split):\n    return pd.DataFrame(data_split, columns=[\"TEXT\", \"Label\"])\n\n# Create DataFrames for train, validation, and test sets\ntrain_data = convert_to_dataframe(data[\"train\"])\nval_data = convert_to_dataframe(data[\"val\"])\ntest_data = convert_to_dataframe(data[\"test\"])\n\n# Display the first few rows of each DataFrame to confirm the format\nprint(\"Training Data:\")\nprint(train_data.head())\n\nprint(\"\\nValidation Data:\")\nprint(val_data.head())\n\nprint(\"\\nTest Data:\")\nprint(test_data.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:24.182567Z","iopub.execute_input":"2024-12-22T13:49:24.182856Z","iopub.status.idle":"2024-12-22T13:49:24.208921Z","shell.execute_reply.started":"2024-12-22T13:49:24.182835Z","shell.execute_reply":"2024-12-22T13:49:24.208140Z"}},"outputs":[{"name":"stdout","text":"Training Data:\n                                                TEXT Label\n0  Sunday afternoon walking through Venice in the...    12\n1  Time for some BBQ and whiskey libations. Chomp...    19\n2  Love love love all these people ï¸ ï¸ ï¸ #friends...     0\n3                               ï¸ ï¸ ï¸ ï¸ @ Toys\"R\"Us      0\n4  Man these are the funniest kids ever!! That fa...     2\n\nValidation Data:\n                                                TEXT Label\n0  A little throwback with my favourite person @ ...     0\n1  glam on @user yesterday for #kcon makeup using...     7\n2  Democracy Plaza in the wake of a stunning outc...    11\n3   Then &amp; Now. VILO @ Walt Disney Magic Kingdom     0\n4               Who never... @ A Galaxy Far Far Away     2\n\nTest Data:\n                                                TEXT Label\n0                                  en Pelham Parkway     0\n1  The calm before...... | w/ sofarsounds @user |...     7\n2  Just witnessed the great solar eclipse @ Tampa...    11\n3  This little lady is 26 weeks pregnant today! E...     0\n4  Great road trip views! @ Shartlesville, Pennsy...     2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"mapping_path = \"/kaggle/working/tweeteval/datasets/emoji/mapping.txt\"\n\n# Load the mapping file into a DataFrame\n# Assuming mapping.txt is formatted as tab-separated, with each line as: Index Emoji Description\nmapping_df = pd.read_csv(mapping_path, sep=\"\\t\", header=None, names=[\"index\", \"emoticons\", \"description\"])\n\n# Add a numeric \"number\" column that matches the row index\nmapping_df[\"number\"] = mapping_df.index\n\n# Rename columns to match the required format\nmapping_df = mapping_df.rename(columns={\"index\": \"Unnamed: 0\"})\n\n# Drop the \"description\" column if it's not needed\nmappings= mapping_df[[\"Unnamed: 0\", \"emoticons\", \"number\"]]\n\n# Display the first few rows to verify\nprint(mappings.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:26.587102Z","iopub.execute_input":"2024-12-22T13:49:26.587445Z","iopub.status.idle":"2024-12-22T13:49:26.605838Z","shell.execute_reply.started":"2024-12-22T13:49:26.587419Z","shell.execute_reply":"2024-12-22T13:49:26.605160Z"}},"outputs":[{"name":"stdout","text":"  Unnamed: 0                      emoticons  number\n0          â¤                    _red_heart_       0\n1          ðŸ˜  _smiling_face_with_hearteyes_       1\n2          ðŸ˜‚       _face_with_tears_of_joy_       2\n3          ðŸ’•                   _two_hearts_       3\n4          ðŸ”¥                         _fire_       4\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train_data.shape, test_data.shape, mappings.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:29.279703Z","iopub.execute_input":"2024-12-22T13:49:29.279996Z","iopub.status.idle":"2024-12-22T13:49:29.285856Z","shell.execute_reply.started":"2024-12-22T13:49:29.279976Z","shell.execute_reply":"2024-12-22T13:49:29.285102Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"((45000, 2), (5000, 2), (20, 3))"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train_length = train_data.shape[0]\ntest_length = test_data.shape[0]\ntrain_length, test_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:30.723968Z","iopub.execute_input":"2024-12-22T13:49:30.724285Z","iopub.status.idle":"2024-12-22T13:49:30.729512Z","shell.execute_reply.started":"2024-12-22T13:49:30.724255Z","shell.execute_reply":"2024-12-22T13:49:30.728617Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(45000, 5000)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import nltk\n\n# Download the 'stopwords' dataset\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\n\nstop_words = stopwords.words(\"english\")\nstop_words[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:32.718061Z","iopub.execute_input":"2024-12-22T13:49:32.718404Z","iopub.status.idle":"2024-12-22T13:49:33.683053Z","shell.execute_reply.started":"2024-12-22T13:49:32.718376Z","shell.execute_reply":"2024-12-22T13:49:33.682257Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['i', 'me', 'my', 'myself', 'we']"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# tokenize the sentences\ndef tokenize(tweets):\n    stop_words = stopwords.words(\"english\")\n    tokenized_tweets = []\n    for tweet in tweets:\n        # split all words in the tweet\n        words = tweet.split(\" \")\n        tokenized_string = \"\"\n        for word in words:\n            # remove @handles -> useless -> no information\n            if word and word[0] != '@' and word not in stop_words:\n                # if a hashtag, remove # -> adds no new information\n                if word[0] == \"#\":\n                    word = word[1:]\n                tokenized_string += word + \" \"\n        tokenized_tweets.append(tokenized_string)\n    return tokenized_tweets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:36.378443Z","iopub.execute_input":"2024-12-22T13:49:36.379005Z","iopub.status.idle":"2024-12-22T13:49:36.383707Z","shell.execute_reply.started":"2024-12-22T13:49:36.378975Z","shell.execute_reply":"2024-12-22T13:49:36.382808Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# translate tweets to a sequence of numbers\ndef encod_tweets(tweets):\n    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', split=\" \", lower=True)\n    tokenizer.fit_on_texts(tweets)\n    return tokenizer, tokenizer.texts_to_sequences(tweets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:38.271671Z","iopub.execute_input":"2024-12-22T13:49:38.271984Z","iopub.status.idle":"2024-12-22T13:49:38.275976Z","shell.execute_reply.started":"2024-12-22T13:49:38.271957Z","shell.execute_reply":"2024-12-22T13:49:38.275147Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# apply padding to dataset and convert labels to bitmaps\ndef format_data(encoded_tweets, max_length, labels):\n    x = pad_sequences(encoded_tweets, maxlen= max_length, padding='post', truncating='post')\n    y = []\n    for emoji in labels:\n        emoji_index = int(emoji)\n        bit_vec = np.zeros(20)\n        bit_vec[emoji_index] = 1\n        y.append(bit_vec)\n    y = np.asarray(y)\n    return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:40.011764Z","iopub.execute_input":"2024-12-22T13:49:40.012065Z","iopub.status.idle":"2024-12-22T13:49:40.016809Z","shell.execute_reply.started":"2024-12-22T13:49:40.012044Z","shell.execute_reply":"2024-12-22T13:49:40.015925Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# create weight matrix from pre trained embeddings\ndef create_weight_matrix(vocab, raw_embeddings):\n    vocab_size = len(vocab) + 1\n    weight_matrix = np.zeros((vocab_size, 300))\n    for word, idx in vocab.items():\n        if word in raw_embeddings:\n            weight_matrix[idx] = raw_embeddings[word]\n    return weight_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:42.130051Z","iopub.execute_input":"2024-12-22T13:49:42.130382Z","iopub.status.idle":"2024-12-22T13:49:42.134776Z","shell.execute_reply.started":"2024-12-22T13:49:42.130354Z","shell.execute_reply":"2024-12-22T13:49:42.133735Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\ndef final_model(weight_matrix, vocab_size, max_length, x_train, y_train, x_val, y_val, embedding_dim=300, lstm_units=128, epochs=5, learning_rate=0.001):\n    # Embedding layer with provided weight matrix\n    embedding_layer = Embedding(\n        input_dim=vocab_size,\n        output_dim=embedding_dim,\n        weights=[weight_matrix],\n        input_length=max_length,\n        trainable=True,\n        mask_zero=False\n    )\n\n    # Model architecture\n    model = Sequential()\n    model.add(embedding_layer)\n\n    model.add(Dropout(0.3))\n    model.add(Dense(32,activation='relu'))\n    model.add((LSTM(64,return_sequences= True)))\n    model.add((LSTM(64,return_sequences= True)))\n    model.add((LSTM(64,return_sequences= True)))\n    model.add((LSTM(64,return_sequences= True)))\n    model.add((LSTM(32)))\n    model.add(Dense(32,activation='relu'))\n    model.add(Dropout(0.3))\n\n    model.add(Dense(20, activation='softmax'))\n\n\n\n    # Compile model with a customizable learning rate\n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n    # Callback to reduce learning rate on plateau\n    lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n\n    # Train model with validation data\n    model.fit(\n        x_train, y_train,\n        epochs=epochs,\n        validation_data=(x_val, y_val),\n        callbacks=[lr_reduction]\n    )\n\n    # Evaluate model on validation set\n    score, acc = model.evaluate(x_val, y_val)\n\n    return model, score, acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:45.436565Z","iopub.execute_input":"2024-12-22T13:49:45.436912Z","iopub.status.idle":"2024-12-22T13:49:45.451089Z","shell.execute_reply.started":"2024-12-22T13:49:45.436882Z","shell.execute_reply":"2024-12-22T13:49:45.450394Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:55.578977Z","iopub.execute_input":"2024-12-22T13:49:55.579297Z","iopub.status.idle":"2024-12-22T13:49:55.582986Z","shell.execute_reply.started":"2024-12-22T13:49:55.579270Z","shell.execute_reply":"2024-12-22T13:49:55.582018Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"tokenized_tweets = tokenize(train_data['TEXT'])\ntokenized_tweets += tokenize(test_data['TEXT'])\nmax_length = math.ceil(sum([len(s.split(\" \")) for s in tokenized_tweets])/len(tokenized_tweets))\ntokenizer, encoded_tweets = encod_tweets(tokenized_tweets)\nmax_length, len(tokenized_tweets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:49:57.086543Z","iopub.execute_input":"2024-12-22T13:49:57.086849Z","iopub.status.idle":"2024-12-22T13:49:59.903416Z","shell.execute_reply.started":"2024-12-22T13:49:57.086826Z","shell.execute_reply":"2024-12-22T13:49:59.902499Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(10, 50000)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"x, y = format_data(encoded_tweets[:train_length], max_length, train_data['Label'])\nlen(x), len(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:50:01.082606Z","iopub.execute_input":"2024-12-22T13:50:01.082935Z","iopub.status.idle":"2024-12-22T13:50:01.236281Z","shell.execute_reply.started":"2024-12-22T13:50:01.082909Z","shell.execute_reply":"2024-12-22T13:50:01.235398Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(45000, 45000)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"x_test, y_test = format_data(encoded_tweets[train_length:], max_length, test_data['Label'])\nlen(x_test), len(y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:50:02.999134Z","iopub.execute_input":"2024-12-22T13:50:02.999484Z","iopub.status.idle":"2024-12-22T13:50:03.025709Z","shell.execute_reply.started":"2024-12-22T13:50:02.999459Z","shell.execute_reply":"2024-12-22T13:50:03.025004Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(5000, 5000)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"vocab = tokenizer.word_index\n# vocab, len(vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T03:32:32.704056Z","iopub.execute_input":"2024-12-21T03:32:32.704393Z","iopub.status.idle":"2024-12-21T03:32:32.708087Z","shell.execute_reply.started":"2024-12-21T03:32:32.704368Z","shell.execute_reply":"2024-12-21T03:32:32.707142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gensim.models.keyedvectors import KeyedVectors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T13:50:07.118897Z","iopub.execute_input":"2024-12-22T13:50:07.119180Z","iopub.status.idle":"2024-12-22T13:50:20.089447Z","shell.execute_reply.started":"2024-12-22T13:50:07.119160Z","shell.execute_reply":"2024-12-22T13:50:20.088768Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"word2vec_model = KeyedVectors.load_word2vec_format('/kaggle/input/swm-wordembed/model_swm_300-6-10-low.w2v', binary=False)\n\n# Create the weight matrix\nweight_matrix = create_weight_matrix(vocab, word2vec_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T03:33:03.733297Z","iopub.execute_input":"2024-12-21T03:33:03.733624Z","iopub.status.idle":"2024-12-21T03:33:42.593523Z","shell.execute_reply.started":"2024-12-21T03:33:03.733597Z","shell.execute_reply":"2024-12-21T03:33:42.592579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, score= final_model(weight_matrix, len(vocab)+1, max_length, x, y, x_test, y_test, epochs = 50)\nmodel, score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:11:42.167541Z","iopub.execute_input":"2024-12-21T04:11:42.167855Z","iopub.status.idle":"2024-12-21T04:15:12.541627Z","shell.execute_reply.started":"2024-12-21T04:11:42.167827Z","shell.execute_reply":"2024-12-21T04:15:12.540915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}